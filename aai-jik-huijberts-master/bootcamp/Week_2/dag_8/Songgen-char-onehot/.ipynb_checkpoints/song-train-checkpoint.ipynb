{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n",
    "import shutil as sh\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "tbCallBack = tf.keras.callbacks.TensorBoard(log_dir='logs', histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "sh.rmtree('logs',ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Show training history --Ferry\n",
    "#  Arguments 1: History object returned by model.fit\n",
    "#            2: Optional string above image..\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_history(history, string = 'Trained Performance'):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    t = f.suptitle(string, fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    nr_epochs = len(history.history['accuracy'])\n",
    "    epoch_list = list(range(1,nr_epochs+1))\n",
    "    \n",
    "    ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_xticks(np.arange(0, nr_epochs, 5))\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "    ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "    ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_xticks(np.arange(0, nr_epochs, 5))\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Loss')\n",
    "    l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abba = pd.read_csv('abba.csv')\n",
    "acdc = pd.read_csv('acdc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abba_corpus = ' ' \n",
    "for index, row in abba.iterrows():\n",
    "    abba_corpus = abba_corpus + str(row['lyrics'])\n",
    "\n",
    "acdc_corpus = ' ' \n",
    "for index, row in acdc.iterrows():\n",
    "    acdc_corpus = acdc_corpus + str(row['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abba_corpus = abba_corpus.lower()\n",
    "acdc_corpus = acdc_corpus.lower()\n",
    "\n",
    "corpus = abba_corpus + acdc_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus contains 72 unique characters.\n",
      "{'\\n': 0, '\\r': 1, ' ': 2, '!': 3, '\"': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, '?': 24, '[': 25, ']': 26, '_': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53, '|': 54, '\\x7f': 55, '\\x83': 56, '\\x9f': 57, '¡': 58, '¤': 59, '¥': 60, '¦': 61, '©': 62, '\\xad': 63, '±': 64, '³': 65, '¶': 66, 'º': 67, '¼': 68, '¿': 69, 'â': 70, 'ã': 71}\n"
     ]
    }
   ],
   "source": [
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it slices, it dices, it makes julienned datasets!\n",
    "# chop up our data into X and y, slice into roughly (num_chars / skip) overlapping 'sentences'\n",
    "# of length sentence_length, and encode the chars\n",
    "# Mischien is sentence_length te kort na 20 een nieuw ckar voorspellen?.\n",
    "sentence_length = 20\n",
    "\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([40, 52, 2, 40, 52, 1, 0, 28, 47, 2, 50, 28, 47, 32, 45, 39, 42, 42, 2, 41],\n",
       " 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[1], y_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 475963 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing X and y...\n"
     ]
    }
   ],
   "source": [
    "# now we need one-hot encoding\n",
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=np.bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's build model.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 20, 64)            35072     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 72)                4680      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 72)                0         \n",
      "=================================================================\n",
      "Total params: 72,776\n",
      "Trainable params: 72,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define our model\n",
    "print(\"Let's build model.\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sentence_length, num_chars), return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump our model architecture to a file so we can load it elsewhere\n",
    "# Find out how to load a model? ,\n",
    "# return_sequences=True\n",
    "architecture = model.to_yaml()\n",
    "with open('model.yaml', 'a') as model_file:\n",
    "    model_file.write(architecture)\n",
    "\n",
    "# Set up checkpoints, and save trained model\n",
    "file_path=\"weights-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "# callbacks = [checkpoint,tbCallBack]\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Epoch 1/10\n",
      "3719/3719 [==============================] - 154s 42ms/step - loss: 1.8882\n",
      "\n",
      "Epoch 00001: loss improved from 2.11028 to 1.88818, saving model to weights-01.hdf5\n",
      "Epoch 2/10\n",
      "3719/3719 [==============================] - 156s 42ms/step - loss: 1.7720\n",
      "\n",
      "Epoch 00002: loss improved from 1.88818 to 1.77200, saving model to weights-02.hdf5\n",
      "Epoch 3/10\n",
      "3719/3719 [==============================] - 148s 40ms/step - loss: 1.6913\n",
      "\n",
      "Epoch 00003: loss improved from 1.77200 to 1.69133, saving model to weights-03.hdf5\n",
      "Epoch 4/10\n",
      "3719/3719 [==============================] - 152s 41ms/step - loss: 1.6307\n",
      "\n",
      "Epoch 00004: loss improved from 1.69133 to 1.63072, saving model to weights-04.hdf5\n",
      "Epoch 5/10\n",
      "3719/3719 [==============================] - 156s 42ms/step - loss: 1.58150s - loss:\n",
      "\n",
      "Epoch 00005: loss improved from 1.63072 to 1.58146, saving model to weights-05.hdf5\n",
      "Epoch 6/10\n",
      "3719/3719 [==============================] - 144s 39ms/step - loss: 1.5395\n",
      "\n",
      "Epoch 00006: loss improved from 1.58146 to 1.53947, saving model to weights-06.hdf5\n",
      "Epoch 7/10\n",
      "3719/3719 [==============================] - 147s 40ms/step - loss: 1.5037\n",
      "\n",
      "Epoch 00007: loss improved from 1.53947 to 1.50370, saving model to weights-07.hdf5\n",
      "Epoch 8/10\n",
      "3719/3719 [==============================] - 147s 40ms/step - loss: 1.4724\n",
      "\n",
      "Epoch 00008: loss improved from 1.50370 to 1.47240, saving model to weights-08.hdf5\n",
      "Epoch 9/10\n",
      "3719/3719 [==============================] - 147s 39ms/step - loss: 1.4452\n",
      "\n",
      "Epoch 00009: loss improved from 1.47240 to 1.44524, saving model to weights-09.hdf5\n",
      "Epoch 10/10\n",
      "3719/3719 [==============================] - 146s 39ms/step - loss: 1.4212\n",
      "\n",
      "Epoch 00010: loss improved from 1.44524 to 1.42122, saving model to weights-10.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Find out how to load the trained checkpoint?\n",
    "# Lets go, action time!\n",
    "#with tf.device('/gpu:1'):\n",
    "    \n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "history = model.fit(X, y, epochs=10, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12345678901234567890"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
